{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Production of indicators for the COVID19 Mobility Task Force\n",
    "\n",
    "In this notebook we produce indicators for the [COVID19 Mobility Task Force](https://github.com/worldbank/covid-mobile-data).\n",
    "\n",
    "[Flowminder](https://covid19.flowminder.org) indicators are produced to increase the availability of comparable datasets across countries, and have been copied without modification from the [Flowminder COVID-19 github repository](https://github.com/Flowminder/COVID-19) (except for the start and end dates). These have been supplemented by a set of *priority* indicators with data for ingestion into the dashboard in this repository.\n",
    "\n",
    "In this notebook we produce indicators in the following four steps:\n",
    "\n",
    "- **Import code**: The code for the aggregation is included in the 'custom_aggregation' and 'flowminder_aggregation' scripts\n",
    "- **Import data**: \n",
    "To set up the data import we need to place the CDR data files into the `data/new/CC/telco/` folder, where we replace `CC` with the country code and `telco` with the company abbreviation. \n",
    "We also need to place csv files with the tower-region mapping and distance matrices into the `data/support-data/CC/telco/geofiles` folder, and then modify the `data/support_data/config_file.py` to specify:\n",
    "    - *geofiles*: the names of the geofiles, \n",
    "    - *country_code*: country code and company abbreviation,\n",
    "    - *telecom_alias*: the path to the `data` folder,\n",
    "    - *data_paths*: the names to the subfolders in `data/new/CC/telco/` that hold the csv files. Simply change this to `[*]` if you didn't create subfolders and want to load all files.\n",
    "    - *dates*: set the start and end date of the data you want to produce the indicators for.\n",
    "    \n",
    "Find more information about the `config_file.py` settings see the [github page](https://github.com/worldbank/covid-mobile-data/tree/master/cdr-aggregation).\n",
    "    \n",
    "- **Run aggregations**: By default, we produce all flowminder and priority indicators. We've included 4 re-tries in case of failure, which we have experienced to help on databricks but is probably irrelevant in other settings. Note that before you can re-run these aggregations, you need to move the csv outputs that have been saved in `data/results/CC/telco/` in previous runs to another folder, else these indicators will be skipped. This prevents you from accidentally overwriting previous results. This way you can also delete the files only for the indicators you want to re-produce, and skip any indicatos you don't want to re-produce.\n",
    "\n",
    "The outcome of this effort will be used to inform policy making using a [mobility indicator dashboard](https://github.com/worldbank/covid-mobile-data/tree/master/dashboard-dataviz)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.setup import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://c0b4ceeab4e9:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fa31e275810>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the configuration for data standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = '../config_file.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(open(config_file).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Basepath: /home/jovyan/work/data\n",
      "Country and company path: zw/***REMOVED***\n",
      "Paths for datafiles: ['mar20/*.csv', 'feb20/*.csv']\n",
      "Geofiles: {'tower_sites': 'zw_***REMOVED***_sites.csv', 'admin2': 'zw_admin2_shapefile.csv', 'admin3': 'zw_admin3_shapefile.csv', 'voronoi': 'zw_voronoi_shapefile.csv', 'admin2_tower_map': 'zw_admin2_tower_map.csv', 'admin3_tower_map': 'zw_admin3_tower_map.csv', 'voronoi_tower_map': 'zw_voronoi_tower_map.csv', 'distances': 'zw_distances_pd_long.csv', 'admin2_incidence': 'zw_admin2_covid_incidence_march30.csv', 'admin2_weight': 'zw_admin2_weight.csv'}\n",
      "Load options: {'seperator': ',', 'header': 'false', 'mode': 'PERMISSIVE', 'datemask': 'dd/MM/yyyy HH:mm:ss'}\n",
      "Load schema: StructType(List(StructField(msisdn,IntegerType,true),StructField(call_datetime,StringType,true),StructField(location_id,StringType,true)))\n",
      "Filenames: {'parquetfile': 'febmar20.parquet'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds = DataSource(datasource_configs)\n",
    "ds.show_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize raw csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds.standardize_csv_files(show=True)\n",
    "# ds.save_as_parquet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds.load_standardized_parquet_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use this in case you want to sample the data and run the code on the sample\n",
    "\n",
    "#ds.sample_and_save(number_of_ids=1000)\n",
    "ds.load_sample('sample_feb_mar2020')\n",
    "ds.parquet_df = ds.sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load geo data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.load_geo_csvs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use this in case you want to cluster the towers and create a distance matrix\n",
    "\n",
    "# ds.create_gpds()\n",
    "# from modules.tower_clustering import *\n",
    "# clusterer = tower_clusterer(ds, 'admin2', 'ID_2')\n",
    "# ds.admin2_tower_map, ds.distances = clusterer.cluster_towers()\n",
    "# clusterer = tower_clusterer(ds, 'admin3', 'ADM3_PCODE')\n",
    "# ds.admin3_tower_map, ds.distances  = clusterer.cluster_towers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use this in case you want to create a voronoi tesselation\n",
    "\n",
    "# from modules.voronoi import *\n",
    "# voronoi = voronoi_maker(ds, 'admin3', 'ADM3_PCODE')\n",
    "# ds.voronoi = voronoi.make_voronoi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flowminder indicators for admin2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped: count_unique_subscribers_per_region_per_day\n",
      "Caching: home_locations\n",
      "Skipped: count_unique_active_residents_per_region_per_day\n",
      "Skipped: count_unique_visitors_per_region_per_day\n",
      "Skipped: count_unique_subscribers_per_region_per_week\n",
      "Skipped: count_unique_active_residents_per_region_per_week\n",
      "Skipped: count_unique_visitors_per_region_per_week\n",
      "Skipped: regional_pair_connections_per_day\n",
      "Skipped: directed_regional_pair_connections_per_day\n",
      "Skipped: total_calls_per_region_per_day\n",
      "Skipped: home_location_counts_per_region\n",
      "Indicators saved.\n"
     ]
    }
   ],
   "source": [
    "agg_flowminder = aggregator(result_stub = '/admin2/flowminder',\n",
    "                            datasource = ds,\n",
    "                            regions = 'admin2_tower_map')\n",
    "\n",
    "agg_flowminder.attempt_aggregation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flowminder indicators for admin3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped: count_unique_subscribers_per_region_per_day\n",
      "Caching: home_locations\n",
      "Skipped: count_unique_active_residents_per_region_per_day\n",
      "Skipped: count_unique_visitors_per_region_per_day\n",
      "Skipped: count_unique_subscribers_per_region_per_week\n",
      "Skipped: count_unique_active_residents_per_region_per_week\n",
      "Skipped: count_unique_visitors_per_region_per_week\n",
      "Skipped: regional_pair_connections_per_day\n",
      "Skipped: directed_regional_pair_connections_per_day\n",
      "Skipped: total_calls_per_region_per_day\n",
      "Skipped: home_location_counts_per_region\n",
      "Indicators saved.\n"
     ]
    }
   ],
   "source": [
    "agg_flowminder = aggregator(result_stub = '/admin3/flowminder',\n",
    "                            datasource = ds,\n",
    "                            regions = 'admin3_tower_map')\n",
    "\n",
    "agg_flowminder.attempt_aggregation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Priority indicators for admin2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped: unique_subscribers_per_day\n",
      "Skipped: percent_of_all_subscribers_active_per_day\n",
      "Skipped: origin_destination_connection_matrix_per_day\n",
      "Skipped: mean_distance_per_day\n",
      "Skipped: week_home_vs_day_location_per_day\n",
      "Skipped: month_home_vs_day_location_per_day\n",
      "Skipped: origin_destination_matrix_time_per_day\n",
      "Skipped: transactions_per_hour\n",
      "Skipped: unique_subscribers_per_hour\n",
      "Skipped: unique_subscriber_home_locations_per_week\n",
      "Skipped: mean_distance_per_week\n",
      "Custom indicators saved.\n"
     ]
    }
   ],
   "source": [
    "agg_custom = custom_aggregator(result_stub = '/admin2/custom',\n",
    "                               datasource = ds,\n",
    "                               regions = 'admin2_tower_map')\n",
    "\n",
    "agg_custom.attempt_aggregation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Priority indicators for admin3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped: unique_subscribers_per_day\n",
      "Skipped: percent_of_all_subscribers_active_per_day\n",
      "Skipped: origin_destination_connection_matrix_per_day\n",
      "Skipped: mean_distance_per_day\n",
      "Skipped: week_home_vs_day_location_per_day\n",
      "Skipped: month_home_vs_day_location_per_day\n",
      "Skipped: origin_destination_matrix_time_per_day\n",
      "Skipped: transactions_per_hour\n",
      "Skipped: unique_subscribers_per_hour\n",
      "Skipped: unique_subscriber_home_locations_per_week\n",
      "Skipped: mean_distance_per_week\n",
      "Custom indicators saved.\n"
     ]
    }
   ],
   "source": [
    "agg_custom = custom_aggregator(result_stub = '/admin3/custom',\n",
    "                            datasource = ds,\n",
    "                            regions = 'admin3_tower_map')\n",
    "\n",
    "agg_custom.attempt_aggregation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled priority indicators for admin2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> File does not exist. Saving: unique_subscribers_per_day\n",
      "--> File does not exist. Saving: percent_of_all_subscribers_active_per_day\n",
      "--> File does not exist. Saving: origin_destination_connection_matrix_per_day\n",
      "--> File does not exist. Saving: mean_distance_per_day\n",
      "--> File does not exist. Saving: week_home_vs_day_location_per_day\n",
      "--> File does not exist. Saving: month_home_vs_day_location_per_day\n",
      "--> File does not exist. Saving: origin_destination_matrix_time_per_day\n",
      "--> File does not exist. Saving: transactions_per_hour\n",
      "--> File does not exist. Saving: unique_subscribers_per_hour\n",
      "--> File does not exist. Saving: unique_subscriber_home_locations_per_week\n",
      "--> File does not exist. Saving: mean_distance_per_week\n",
      "--> Renaming: unique_subscribers_per_day\n",
      "--> Renaming: percent_of_all_subscribers_active_per_day\n",
      "--> Renaming: origin_destination_connection_matrix_per_day\n",
      "--> Renaming: mean_distance_per_day\n",
      "--> Renaming: week_home_vs_day_location_per_day\n",
      "--> Renaming: month_home_vs_day_location_per_day\n",
      "--> Renaming: origin_destination_matrix_time_per_day\n",
      "--> Renaming: transactions_per_hour\n",
      "--> Renaming: unique_subscribers_per_hour\n",
      "--> Renaming: unique_subscriber_home_locations_per_week\n",
      "--> Renaming: mean_distance_per_week\n",
      "Custom indicators saved.\n"
     ]
    }
   ],
   "source": [
    "agg_custom = scaled_aggregator(result_stub = '/admin2/scaled',\n",
    "                               datasource = ds,\n",
    "                               regions = 'admin2_tower_map')\n",
    "\n",
    "agg_custom.attempt_aggregation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled priority indicators for admin3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> File does not exist. Saving: unique_subscribers_per_day\n",
      "--> File does not exist. Saving: percent_of_all_subscribers_active_per_day\n",
      "--> File does not exist. Saving: origin_destination_connection_matrix_per_day\n",
      "--> File does not exist. Saving: mean_distance_per_day\n",
      "--> File does not exist. Saving: week_home_vs_day_location_per_day\n",
      "--> File does not exist. Saving: month_home_vs_day_location_per_day\n",
      "--> File does not exist. Saving: origin_destination_matrix_time_per_day\n",
      "--> File does not exist. Saving: transactions_per_hour\n",
      "--> File does not exist. Saving: unique_subscribers_per_hour\n"
     ]
    }
   ],
   "source": [
    "agg_custom = scaled_aggregator(result_stub = '/admin3/scaled',\n",
    "                            datasource = ds,\n",
    "                            regions = 'admin3_tower_map')\n",
    "\n",
    "agg_custom.attempt_aggregation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Produce script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to script *.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
